# ğŸ“š Paper Implementations

This repository provides `from-scratch implementations` of core models and architectures introduced in seminal machine learning papers, using `PyTorch`. It serves as both an educational and reference resource for understanding foundational and cutting-edge approaches across a `variety of domains`.

Below is a list of the `research papers` currently implemented in this repository, with proper attribution.

| Paper Title                                                                                                    | Domain          | Citation                 |
| -------------------------------------------------------------------------------------------------------------- | --------------- | ------------------------ |
| [Attention is All You Need](https://arxiv.org/abs/1706.03762)                                                  | NLP             | Vaswani et al., 2017     |
| [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) | Computer Vision | Dosovitskiy et al., 2020 |
| [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)              | NLP             | Su et al., 2021          |


## Transformer Architecture (Attention is All You Need)

<img width="556" height="731" alt="image" src="https://github.com/user-attachments/assets/0d434c8d-d2ab-494d-8895-cf504a5a4417" />

## Vision Transformer Architecture (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale)

<img width="908" height="460" alt="image" src="https://github.com/user-attachments/assets/fb251aae-a183-4961-9133-3d0ee60b15a6" />

## RoPE (RoFormer: Enhanced Transformer with Rotary Position Embedding)

<img width="1026" height="583" alt="image" src="https://github.com/user-attachments/assets/87ffe32f-e1d4-4fa4-834a-2d260b1d0939" />



## ğŸ—‚ï¸ Repository Files Structure 

The implementations in this repository are built using a layered and modular approach, starting from fundamental components and gradually assembling them into `complete, functional architectures`. This design promotes reusability, understanding, and easy experimentation with different model variants.

```bash

transformer-implementation-with-pytorch
â”‚
â”œâ”€â”€ transformer/                      # Contains Transformer model and components
â”‚   â”œâ”€â”€ __init__.py                  # Makes 'transformer' a Python package
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                       # Final transformer model
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Makes 'model' a Python package
â”‚   â”‚   â””â”€â”€ transformer.py           # Finished Transformer architecture
â”‚   â”‚
â”‚   â””â”€â”€ model_components/            # Core components of the Transformer model
â”‚       â”œâ”€â”€ blocks/                  # Encoder and Decoder blocks
â”‚       â”‚   â”œâ”€â”€ __init__.py          # Makes 'blocks' a Python package
â”‚       â”‚   â”œâ”€â”€ encoder.py           # Encoder block implementation
â”‚       â”‚   â””â”€â”€ decoder.py           # Decoder block implementation
â”‚       â”‚
â”‚       â”œâ”€â”€ embedding/               # Embedding layers
â”‚       â”‚   â”œâ”€â”€ __init__.py          # Makes 'embedding' a Python package
â”‚       â”‚   â””â”€â”€ input_embedding.py   # Input token + positional embedding
â”‚       â”‚
â”‚       â””â”€â”€ layers/                  # Core sub-layers of transformer blocks
â”‚           â”œâ”€â”€ __init__.py          # Makes 'layers' a Python package
â”‚           â”œâ”€â”€ feed_forward_nn.py   # Position-wise feed-forward network
â”‚           â””â”€â”€ multi_head_attention.py  # Multi-head self-attention mechanism
â”‚
â”œâ”€â”€ vision_transformer/              # Vision Transformer implementation
â”‚   â”œâ”€â”€ __init__.py                  # Makes 'vision_transformer' a Python package
â”‚   â”œâ”€â”€ implementation.py            # Vision Transformer code
â”‚   â””â”€â”€ train_mnist.ipynb            # Training notebook for MNIST
â”œâ”€â”€ roformer/                         # RoFormer implementation
â”‚   â”œâ”€â”€ __init__.py                  
â”‚   â”œâ”€â”€ rope.py                       # Rotary Positional Encoding
â”‚   â””â”€â”€ transformer_with_rope.py      # Transformer model with RoPE
â”œâ”€â”€ model_test.py                    # Script for testing the model
â”œâ”€â”€ requirements.txt                 # Python dependencies for the project
â”œâ”€â”€ README.md                       # Project overview and documentation
â”œâ”€â”€ .gitignore                      # Files/folders to ignore in version control
â””â”€â”€ __init__.py                     # Makes the root directory a Python package


``` 






